{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8736849e",
   "metadata": {},
   "source": [
    "# Linear models for regression\n",
    "\n",
    "## Goal of regression\n",
    "Predict the value of one or more continous variables based on input variables. \n",
    "\n",
    "## What are linear regression models?\n",
    "Simplest form of linear regression model is of course a linear function. But, much better results can be obtained by using **linear combinations of a fixed set of functions, known as basis functions**. Even though some of the basis functions are nonlinear w.r.t. the input variables, **they are linear function of the parameters**.\n",
    "\n",
    "\n",
    "\n",
    "## How does regression make predictions?\n",
    "Given a training data set which includes observation and target values, the goal is to predict target value for a new observation.\n",
    "\n",
    "Model distribution p(target|input) represents the uncertainty in predictions and allows minimizing expected loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96afab",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec8d32",
   "metadata": {},
   "source": [
    "## Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e0fd6",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "$$ y(x, w) = w_0 + w_1x_1 + ... + w_Dx_D $$\n",
    "\n",
    "The key property of linear regression is that it is not only a linear function of the parameters, but also of the input variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddcf1a",
   "metadata": {},
   "source": [
    "### Other basis functions\n",
    "\n",
    "But in linear methods, we want to include linear combinations of fixed nonlinear functions (basis functions) of the input variables, which take the form:\n",
    "\n",
    "$$ y(x, w) = w_0 + \\sum_{j=1}^{M-1}w_j\\phi_j(x) $$\n",
    "\n",
    "The parameter $w_{0}$ is called bias parameter and allows for any fixed offset in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef32b0a",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "$$ \\phi_j(x) = x^j $$\n",
    "\n",
    "One of the limitations of polynomial regression can be that it is a global function of the input variable. When it changes in one region of input space it affects all other regions. \n",
    "\n",
    "This can be solved by dividing the input space into regions and fit different polynomial in each region -> spline functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c74cf9",
   "metadata": {},
   "source": [
    "### Gaussian basis functions\n",
    "$$ \\phi_j(x) = exp \\left\\{ \\frac{(x - \\mu_j)^2}{2s^2} \\right\\} $$\n",
    "\n",
    "$\\mu$ is responsible for the locations of the basis functions in the input space and the parameter s for their spatial scale. even though names 'Gaussian' they do not have to have probabilistic interpretations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed449fc9",
   "metadata": {},
   "source": [
    "### Sigmoidal basis function\n",
    "\n",
    "$$ \\phi_j(x) = \\sigma \\left( \\frac{x - \\mu_j}{s} \\right) $$\n",
    "\n",
    "where the sigmoid function is defined by $ \\sigma(a) = \\frac{1}{1+ exp(-a)}$. Similarly, we can use tanh function, because $tanh(a) = 2\\sigma(a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a343f39",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86821a54",
   "metadata": {},
   "source": [
    "## Maximum likelihood and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3872d5",
   "metadata": {},
   "source": [
    "In an essence - **minimizing the sum-of-squared error function is equivalent to maximizing the likelihood under the assumption that the noise follows a Gaussian distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ec4a1",
   "metadata": {},
   "source": [
    "_Note: primary goal in the supervised learning is to model the relationship between inputs and outputs, rather than modeling the probability distribution of the inputs p(x) themselves._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8af9f5",
   "metadata": {},
   "source": [
    "A function can be fit to the data by minimizing sum-of-square error function. \n",
    "\n",
    "1. Assume the target variable t is given by a deterministic function y(x, w) with additive Gaussian noise:\n",
    "$$ t = y(x,w) + \\epsilon $$\n",
    "\n",
    "\n",
    "2. Thus, it can be written as: \n",
    "$$ p(t|x,w, \\beta) = N \\left( t|y(x,w), \\beta^{-1} \\right) $$\n",
    "which basically implies what kind of probability density function the target values follow.\n",
    "\n",
    "\n",
    "3. If a data set inputs $X = {x_1, ... x_N}$ with corresponding target values $t_1, ..., t_N$ are assumed to be drawn independently from the distribution, then the following is the **likelihood function of the adjustable parameters w and $\\beta$**:\n",
    "$$p(t|X, w, \\beta) = \\prod_{n=1}^{N} \\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1} )$$\n",
    "    \n",
    "    \n",
    "4. Next, we take the logarithm of the likelihood function because it simplifies computations:\n",
    "$$ln(pt|w, \\beta) = \\sum_{n=1}^N ln \\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1} ) = \\frac{N}{2}ln \\beta - \\frac{N}{2}ln(2\\pi) - \\beta E_D(w)$$\n",
    "\n",
    "where $E_D$ is the sum-of-square error is defined by:\n",
    "$$E_d(W) = \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - w^T\\phi(x_n)\\right)^2$$\n",
    "\n",
    "\n",
    "5. To find the maximum likelihood estimate (MLE) for the parameters w and $\\beta$ is equivalent to minimizing the negative log-likelihood:\n",
    "$$-lnp(t|w, \\beta) = -\\frac{N}{2}ln \\beta + \\frac{N}{2}ln(2\\pi) + \\beta E_D(w)$$\n",
    "It is allowed to drop first two expressions as they are scaling factors, and while they are relevant for determining $\\beta$, they do not change the maximization problem with respect to w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c668521",
   "metadata": {},
   "source": [
    "By maximizing the log likelihood function w.r.t $\\beta$ or w.r.t. w it is possible to derive their values.\n",
    "- the bias $w$ compensates for the difference between the averages of the target values and the wieghted sum of the averages of the basis function values.\n",
    "- the noise precision parameter $\\beta$ is given by residual variance of the target values around the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b1d12",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce404c",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569633d5",
   "metadata": {},
   "source": [
    "It's technique where data points are considered one at a time and the model parameters are updated after each such presentation. \n",
    "\n",
    "When applied to the sum of squares error function, it gives:\n",
    "$$w^{(\\tau +1)} = w^{(\\tau)} - \\eta (t_n - w^{(\\tau)T}\\phi_n)\\phi_n$$\n",
    "\n",
    "where $\\eta$ is the learning rate parameter. This is also known as least-mean-squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b59db",
   "metadata": {},
   "source": [
    "______________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
