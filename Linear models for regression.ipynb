{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8736849e",
   "metadata": {},
   "source": [
    "# Linear models for regression\n",
    "\n",
    "## Goal of regression\n",
    "Predict the value of one or more continous variables based on input variables. \n",
    "\n",
    "## What are linear regression models?\n",
    "Simplest form of linear regression model is of course a linear function. But, much better results can be obtained by using **linear combinations of a fixed set of functions, known as basis functions**. Even though some of the basis functions are nonlinear w.r.t. the input variables, **they are linear function of the parameters**.\n",
    "\n",
    "\n",
    "\n",
    "## How does regression make predictions?\n",
    "Given a training data set which includes observation and target values, the goal is to predict target value for a new observation.\n",
    "\n",
    "Model distribution p(target|input) represents the uncertainty in predictions and allows minimizing expected loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96afab",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec8d32",
   "metadata": {},
   "source": [
    "## Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e0fd6",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "$$ y(x, w) = w_0 + w_1x_1 + ... + w_Dx_D $$\n",
    "\n",
    "The key property of linear regression is that it is not only a linear function of the parameters, but also of the input variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddcf1a",
   "metadata": {},
   "source": [
    "### Other basis functions\n",
    "\n",
    "But in linear methods, we want to include linear combinations of fixed nonlinear functions (basis functions) of the input variables, which take the form:\n",
    "\n",
    "$$ y(x, w) = w_0 + \\sum_{j=1}^{M-1}w_j\\phi_j(x) $$\n",
    "\n",
    "The parameter $w_{0}$ is called bias parameter and allows for any fixed offset in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef32b0a",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "$$ \\phi_j(x) = x^j $$\n",
    "\n",
    "One of the limitations of polynomial regression can be that it is a global function of the input variable. When it changes in one region of input space it affects all other regions. \n",
    "\n",
    "This can be solved by dividing the input space into regions and fit different polynomial in each region -> spline functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c74cf9",
   "metadata": {},
   "source": [
    "### Gaussian basis functions\n",
    "$$ \\phi_j(x) = exp \\left\\{ \\frac{(x - \\mu_j)^2}{2s^2} \\right\\} $$\n",
    "\n",
    "$\\mu$ is responsible for the locations of the basis functions in the input space and the parameter s for their spatial scale. even though names 'Gaussian' they do not have to have probabilistic interpretations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed449fc9",
   "metadata": {},
   "source": [
    "### Sigmoidal basis function\n",
    "\n",
    "$$ \\phi_j(x) = \\sigma \\left( \\frac{x - \\mu_j}{s} \\right) $$\n",
    "\n",
    "where the sigmoid function is defined by $ \\sigma(a) = \\frac{1}{1+ exp(-a)}$. Similarly, we can use tanh function, because $tanh(a) = 2\\sigma(a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a343f39",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86821a54",
   "metadata": {},
   "source": [
    "## Maximum likelihood and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3872d5",
   "metadata": {},
   "source": [
    "In an essence - **minimizing the sum-of-squared error function is equivalent to maximizing the likelihood under the assumption that the noise follows a Gaussian distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ec4a1",
   "metadata": {},
   "source": [
    "_Note: primary goal in the supervised learning is to model the relationship between inputs and outputs, rather than modeling the probability distribution of the inputs p(x) themselves._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8af9f5",
   "metadata": {},
   "source": [
    "A function can be fit to the data by minimizing sum-of-square error function. \n",
    "\n",
    "1. Assume the target variable t is given by a deterministic function y(x, w) with additive Gaussian noise:\n",
    "$$ t = y(x,w) + \\epsilon $$\n",
    "\n",
    "\n",
    "2. Thus, it can be written as: \n",
    "$$ p(t|x,w, \\beta) = N \\left( t|y(x,w), \\beta^{-1} \\right) $$\n",
    "which basically implies what kind of probability density function the target values follow.\n",
    "\n",
    "\n",
    "3. If a data set inputs $X = {x_1, ... x_N}$ with corresponding target values $t_1, ..., t_N$ are assumed to be drawn independently from the distribution, then the following is the **likelihood function of the adjustable parameters w and $\\beta$**:\n",
    "$$p(t|X, w, \\beta) = \\prod_{n=1}^{N} \\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1} )$$\n",
    "    \n",
    "    \n",
    "4. Next, we take the logarithm of the likelihood function because it simplifies computations:\n",
    "$$ln(pt|w, \\beta) = \\sum_{n=1}^N ln \\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1} ) = \\frac{N}{2}ln \\beta - \\frac{N}{2}ln(2\\pi) - \\beta E_D(w)$$\n",
    "\n",
    "where $E_D$ is the sum-of-square error is defined by:\n",
    "$$E_d(W) = \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - w^T\\phi(x_n)\\right)^2$$\n",
    "\n",
    "\n",
    "5. To find the maximum likelihood estimate (MLE) for the parameters w and $\\beta$ is equivalent to minimizing the negative log-likelihood:\n",
    "$$-lnp(t|w, \\beta) = -\\frac{N}{2}ln \\beta + \\frac{N}{2}ln(2\\pi) + \\beta E_D(w)$$\n",
    "It is allowed to drop first two expressions as they are scaling factors, and while they are relevant for determining $\\beta$, they do not change the maximization problem with respect to w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c668521",
   "metadata": {},
   "source": [
    "By maximizing the log likelihood function w.r.t $\\beta$ or w.r.t. w it is possible to derive their values.\n",
    "- the bias $w$ compensates for the difference between the averages of the target values and the wieghted sum of the averages of the basis function values.\n",
    "- the noise precision parameter $\\beta$ is given by residual variance of the target values around the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b1d12",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce404c",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569633d5",
   "metadata": {},
   "source": [
    "It's technique where data points are considered one at a time and the model parameters are updated after each such presentation. \n",
    "\n",
    "When applied to the sum of squares error function, it gives:\n",
    "$$w^{(\\tau +1)} = w^{(\\tau)} - \\eta (t_n - w^{(\\tau)T}\\phi_n)\\phi_n$$\n",
    "\n",
    "where $\\eta$ is the learning rate parameter. This is also known as least-mean-squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b59db",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb297cc",
   "metadata": {},
   "source": [
    "## Regularized least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a7052",
   "metadata": {},
   "source": [
    "Adding a regularization term to an error function helps to control overfitting. The regularized form of error function takes the form:\n",
    "$$E_D(w) + \\lambda E_W(w)$$\n",
    "\n",
    "where $\\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(w)$ and the regularization term $E_W(w)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f438344",
   "metadata": {},
   "source": [
    "### Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704045d",
   "metadata": {},
   "source": [
    "A simple form of regularizer can be for example sum-of-squares of the weight vector elements: $$E_W(w) = \\frac{1}{2}w^Tw$$\n",
    "\n",
    "If we apply it to the sum-of-squares error then the total error function becomes:\n",
    "\n",
    "$$\\frac{1}{2} \\sum_{n=1}^N \\left(t_n - w^T\\phi(x_n)\\right)^2 + \\frac{\\lambda}{2}w^Tw$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e32b30",
   "metadata": {},
   "source": [
    "This is called weight decay, because it encourages weight values to decay towards zero. It's an example of a parameter shrinkage method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2db357",
   "metadata": {},
   "source": [
    "### Lasso / $L_1$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865f773",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2} \\sum_{n=1}^N \\left(t_n - w^T\\phi(x_n)\\right)^2 + \\lambda \\sum_{j=1}^{p}|w_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fbf69",
   "metadata": {},
   "source": [
    "The $\\sum_{j=1}^{p}|w_j|$ is the $L_1$ norm which encourages sparsity in w. That means that if $\\lambda$ is sufficiently large, then some of the coefficients $w_j$ are driven to exactly zero. (Feature selection) This makes LASSO useful for models with many irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e91dd",
   "metadata": {},
   "source": [
    "### Ridge / $L_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efee92f",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2} \\sum_{n=1}^N \\left(t_n - w^T\\phi(x_n)\\right)^2 + \\lambda \\sum_{j=1}^{p}w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d49d9b",
   "metadata": {},
   "source": [
    "Ridge regression modifies the sum of square errors by adding an L2 penalty, which is the sum of squared weights. Thisencourages encourages smaller weight values. Unlike Lasso, Ridge shrinks weights close to zero but does not force them to be exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6e028",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede1a3f",
   "metadata": {},
   "source": [
    "## What to do if there are multiple target variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe3b80",
   "metadata": {},
   "source": [
    "### Univariate Approach\n",
    "\n",
    "Idea is to train a separate regression model for each target variable.  This leads to multiple, independent regression problems. But, ignores the relationships between target variables.\n",
    "\n",
    "### Multivariate Approach\n",
    "Extend the traditional regression by treating both the input and output as matrices. Instead of predicting a single target variable, predict multiple dependent variables simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10617c08",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fc90a",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fcb81",
   "metadata": {},
   "source": [
    "It describes the balance between underfitting and overfitting and helps explain why models may generalize well or poorly to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cd7eb",
   "metadata": {},
   "source": [
    "The model's expected loss can be decomposed into three components:\n",
    "\n",
    "$$Total Error = Bias^2 + Variance + Noise$$\n",
    "\n",
    "### Bias\n",
    "- Represents the extent to which the average prediction over all data sets\n",
    "differs from the desired regression function, which means that it indicates how wrong the model is on average.\n",
    "- High bias means the model makes strong assumptions about the data\n",
    "- Leads to **underfitting** - oversimplified models.\n",
    "\n",
    "### Variance \n",
    "- Measures the extent to which the solutions for individual data sets vary around their average.\n",
    "– Shows how much the model’s predictions change when trained on different data samples.\n",
    "- High variance means the model is too sensitive to training data.\n",
    "- Leads to **overfitting** - memorizing noise instead of learning patterns.\n",
    "\n",
    "The goal is to find a sweet spot between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49082b49",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/2560px-Bias_and_variance_contributing_to_total_error.svg.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedf28c",
   "metadata": {},
   "source": [
    "**If bias is too high:**\n",
    "- Use a more complex model\n",
    "- Add more features.\n",
    "- Reduce regularization.\n",
    "\n",
    "**If variance is too high:**\n",
    "- Use a simpler model.\n",
    "- Get more training data.\n",
    "- Apply regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329a5a8",
   "metadata": {},
   "source": [
    "However, bias-variance trade-off is of limited practical value, because it is based on averages with respect to ensembles of data sets, whereas in practice we have only the single observed data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aec1ee",
   "metadata": {},
   "source": [
    "______________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
